<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>What is the proper level of I/O abstraction in world models (that generate simulated rollouts)?</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            font-size: 14px;
            line-height: 1.2;
            margin: 10px;
            padding: 0;
            background-color: #f9f9f9;
            color: #333;
            max-width: 88%;
        }
        h1 {
            color: #444;
        }
        .date {
            color: #666;
            font-size: x-small
        }
        img {
            max-width: 95%;
            height: auto;
        }
    </style>
</head>
<body>
    <article>
        <p><a href="../index.html"><< back to home</a></p>

        <h1>What is the proper level of I/O abstraction in world models (that generate simulated rollouts)?</h1>
        <p class="date">Sonny George<br>January 10, 2025</p>

        <p>As excitement builds around the idea of 'planning with simulated rollouts,' an open question remains regarding the most useful methods and degrees of abstraction in world model I/O spaces.</p>

        <p>Video is an intuitive I/O space for generating simulated rollouts and contains high levels of detail/information (a picture is worth a thousand words). If we can generate realistic video, then simulating rollouts with video will indeed be viable. There is a lot of justified hype here. (See e.g., <a href="https://universal-simulator.github.io/unisim/">UniSim</a> or <a href="https://sites.google.com/view/genie-2024/">Genie</a>)</p>

        <p>However, could much of video's pixel-by-pixel details be unnecessary for basic actionâ€”creating, at best, potentially expensive prediction overhead, and at worst, SGD distraction that prevents the learning of anything useful for long-tail and OOD scenarios? For instance, video models are often great at generating small details (e.g., textures) while struggling with higher-level dynamics (e.g., maintaining the physical coherence of a dynamic object).</p>

        <p><span style="font-weight: bold;">Can we map internet-scale video data into a more appropriately abstracted representation space with minimal superfluous detail for basic action?</span> Recently, Yann Lecun seems to be of <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf">the opinion</a> that doing so is not only feasible, but also necessary for "autonomous machine intelligence" (see Lecun et. al's <a href="https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/">V-JEPA</a> or alternatively, as an example of an <span style="font-style: italic;">RL</span> algorithm that simulates rollouts in a latent space, <a href="https://danijar.com/project/daydreamer/">DayDreamer</a>).</p>
            
        <p>Roboticists are essentially doing a version of this when they, before learning control policies, distill image/lidar data into assortments of pertinent 3d representations (maps, way points, point clouds, voxels, etc). Similarly, could action trajectories in internet-scale video be converted into consistently sensical and sufficiently informative 3d representations (e.g., with techniques like 3d scene reconstruction and pose estimation)?</p>

        <p>Whether mapping video data for world model training to (1) (an assortment of?) 3d representations or (2) some other representation space with a theoretically higher (relevant-) signal-to-noise ratio, what stands to be gained/lost?</p>
            
        <p>Could mapping massive video datasets into 3d trajectory representations and using them to train next-state-predicting world models have other downstream benefits? E.g.,</p>
            <p style="margin:5px;margin-left:20px">(1) Easier inference of embodiment-specific controls (by generating outputs that are <span style="font-style: italic;">already</span> 3d representations of target poses)
            <br>(2) Cross-embodiment positive learning transfer? (perhaps the generation of any-embodiment outputs could be learned by conditioning generation on an embodiment specification<span style="color: red;">*</span>)
            <br>(3) Ability to incorporate physics-informed inductive biases (e.g., loss penalties for volumetric or joint-range impossibilities)
            <br>(4) Enablement of interpretable safety measures (e.g., preventing generations with potential collisions that approach a force threshold)
            </p>
        
        <p style="margin: 20px;"></p>
        <img src="../assets/imgs/diagram_of_abstraction_levels_for_world_models.png" alt="Diagram of abstraction levels for world models">
    </article>
</body>
</html>
